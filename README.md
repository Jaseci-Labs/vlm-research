# ğŸ”¬ Jac Vision VLM Experiment Framework

A modular, future-proof experiment framework for research and development on **Visual Language Models (VLMs)**. Built with **Hydra**, **Weights & Biases**, and extensible YAML-based configurations â€” ideal for conducting prompting experiments, fine-tuning comparisons, training image impact studies, and more.

---

## ğŸš€ Features

- âœ… Modular config-based experiment control (via [Hydra](https://hydra.cc))
- âœ… W&B integration for grouped runs, logging, and comparisons
- âœ… Supports **PEFT vs full fine-tuning**, image ablation, and prompting studies
- âœ… Clean architecture for easily extending models, datasets, and tasks
- âœ… Hydra sweep support for systematic hyperparameter tuning
- âœ… Ready for scaling to large experiments

---

## ğŸ“ Project Structure

```
vlm-research/
â”œâ”€â”€ main.py                    # Entry point for training/sweeps
â”œâ”€â”€ train.py                   # Training logic (Hydra-driven)
â”œâ”€â”€ evaluate.py                # Prompt evaluation logic
â”œâ”€â”€ runner.py                  # Experiment dispatcher
â”œâ”€â”€ experiment_registry.yaml   # Maps experiment types to logic
â”‚
â”œâ”€â”€ configs/                   # Modular Hydra config directory
â”‚   â”œâ”€â”€ config.yaml            # Base config
â”‚   â”œâ”€â”€ experiment/            # Experiment definitions
â”‚   â”œâ”€â”€ sweep/                 # Sweep configs (e.g. lr, batch size)
â”‚   â”œâ”€â”€ model/                 # Model-specific configs
â”‚   â”œâ”€â”€ training/              # Training parameters
â”‚   â”œâ”€â”€ dataset/               # Dataset-specific configs
â”‚
â”œâ”€â”€ utils/                     # Helper functions
â”‚   â”œâ”€â”€ wandb_utils.py         # W&B init, naming, grouping
â”‚   â””â”€â”€ loaders.py             # Model & dataset loaders
â”‚
â”œâ”€â”€ outputs/                   # Hydra run outputs
â”œâ”€â”€ logs/                      # Optional log outputs
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Installation

```bash
git clone https://github.com/yourusername/vlm-research.git
cd vlm-research
pip install -r requirements.txt
```

---

## ğŸ§ª Running Experiments

### â¤ Run a single experiment

```bash
python main.py experiment=peft_vs_full model=qwen7b dataset=car_damage
```

### â¤ Run a hyperparameter sweep (Hydra multirun)

```bash
python main.py -m sweep.lr=1e-5,5e-5,1e-4 sweep.batch_size=4,8,16
```

Or with YAML:

```bash
python main.py -m +sweep=lr_vs_batch
```

---

## ğŸ“Š W&B Logging

Each run will automatically log:

- Metrics
- Config values
- Grouped by experiment type
- Custom run names (e.g., `qwen7b_1e-5_8batch`)

W&B project and experiment name are customizable via config.

---

## ğŸ§© Adding New Experiments

1. Add new YAML to `configs/experiment/`
2. Update `experiment_registry.yaml`
3. Add handling logic to `runner.py`

---

## ğŸ§  Ideal Use Cases

- VLM-based insurance prediction
- Prompt vs. finetuning comparison studies
- Training data ablation experiments
- Academic/research reproducibility

---

## ğŸ“Œ TODO / Coming Soon

- [ ] Hugging Face model auto-loading
- [ ] Fine-tuning via Unsloth or LoRA
- [ ] Built-in evaluation metrics for VQA, OCR, etc.
- [ ] Frontend integration with Jac Vision UI
- [ ] On-the-fly model training via backend API

---

## ğŸ¤ Contributing

We're building this as a flexible backend for the **Jac Vision** no-code platform and for visual AI experimentation. If you're a researcher, engineer, or builder â€” PRs and suggestions are welcome.

---

## ğŸ“„ License

MIT â€” free to use, extend, and share.
